<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="google-site-verification" content="MmV_OLj--Ry5XArea1wMvMsWhdieJvLHUytRp0NkmBE" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="AiLab123">
    
    <title>
        
            Pytorch深度学习教程(3):Mnist图像分类TorchServe部署 |
        
        AiLab123
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/ai.svg">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/css/font-awesome.min.css">
    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"yqstar.github.io","root":"/","language":"zh-CN","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/ai.svg","favicon":"/images/ai.svg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"人往往会对未知的事情产生恐惧，因为结局是未知的。"},"scroll":{"progress_bar":{"enable":false},"percent":{"enable":true}}},"local_search":{"enable":true,"preload":false},"code_copy":{"enable":true,"style":"mac"},"pjax":{"enable":true},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="AndrewYq" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                AiLab123
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/links"
                            >
                                友链
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/links">友链</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">Pytorch深度学习教程(3):Mnist图像分类TorchServe部署</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/ai.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">AiLab123</span>
                        
                            <span class="author-label">AI调参师</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2022-06-06 18:05:23</span>
        <span class="mobile">2022-06-06 18:05</span>
    </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Pytorch-Tutorial/">Pytorch_Tutorial</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>2.4k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>11 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>经过前 <a href="https://yqstar.github.io/tags/Pytorch-Tutorial/">两篇博客</a> 学习，我们已可使用CNN模型完成Mnist手写数字分类模型，对于算法从数据处理、模型构建、模型训练和评估链路有初步认知。但工业可能需要部署离线在线模型用于提供模型推理服务，所谓模型推理服务是指在系统配置训练完成机器学习模型，以便其可接受新的输入并将推理结果返回给系统。</p>
<p>其次，虽然很多大厂都会有封装好部署平台供算法人员便捷配置，但是学习中对于完整的工程链路开发对于个人能力建设也是非常重要的，而不是仅仅作为一颗螺丝钉，如何实现从Demo模型转换成线上模型推理服务部署，对于个人的正向反馈也是非常有意义的。</p>
<p>那么针对 <em>PyTorch</em> 训练好 Demo 模型，如何部署到生产环境用于提供模型推理服务呢？部署形式非常多样，其中 TorchServe 是 <a class="link" target="_blank" rel="noopener" href="https://pytorch.org/serve/index.html">PyTorch开源项目<i class="fas fa-external-link-alt"></i></a> 部分，是AWS和Facebook合作开发的用于部署Pytorch的模型，对于算法工程师是相当友好的。本章介绍如何使用TorchServe完成PyTorch模型的部署和调用。</p>
<h2 id="TorchServe简介"><a href="#TorchServe简介" class="headerlink" title="TorchServe简介"></a>TorchServe简介</h2><p>Torchserve是PyTorch的首选模型部署解决方案。它允许为模型公开一个可供直接访问或者应用程序访问的WebAPI，借助TorchServe，PyTorch用户可以更快地将其模型应用于生产，而无需编写自定义代码，此外，TorchServe将工程开发和算法开发进行解耦，算法工程师主要完成数据Process和模型构建这一擅长领域，其他的多模型服务、A/B测试的版本控制、监视指标以及应用程序集成RESTful都已封装好。</p>
<blockquote>
<p>TorchServe is a performant, flexible and easy to use tool for serving PyTorch eager mode and torschripted models.</p>
</blockquote>
<p>官网介绍可看出：TorchServe是一款性能好、灵活性好、易使用的工具，其次可部署模型类型是Pytorch的Eager模式和Script模式模型。</p>
<p>TorchServe框架主要分为四个部分：Frontend是TorchServe的请求和响应的处理部分；WorkerProcess 指的是一组运行的模型实例，可以由管理API设定运行的数量；Model Store是模型存储加载的地方；Backend用于管理Worker Process，具体可参考下图里。</p>
<p><img src="/2022/06/06/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-3-Mnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBTorchServe%E9%83%A8%E7%BD%B2/ts_frame.png" alt="ts_frame"></p>
<h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><p>本人使用 Windows11+WSL2+Ubuntu 环境进行部署。</p>
<h3 id="Conda配置"><a href="#Conda配置" class="headerlink" title="Conda配置"></a>Conda配置</h3><p><a class="link" target="_blank" rel="noopener" href="https://github.com/pytorch/serve">官网要求<i class="fas fa-external-link-alt"></i></a>：Python Version &gt;= 3.8，本文使用Conda管理深度学习环境，具体使用可参考之前的博文：<a href="https://yqstar.github.io/2022/05/01/Windows%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8Conda%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/">深度学习管理配置</a>。</p>
<p><img src="/2022/06/06/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-3-Mnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBTorchServe%E9%83%A8%E7%BD%B2/python_version.png" alt="python_version"></p>
<p>可使用下述命令创建Conda的Python环境（python版本为3.8，环境名为ts_ENV）和激活指定环境(ts_env)。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name ts_env python=3.8</span><br><span class="line">conda activate ts_env</span><br></pre></td></tr></table></figure>
<h3 id="TS源码安装"><a href="#TS源码安装" class="headerlink" title="TS源码安装"></a>TS源码安装</h3><p>可参考官网TS安装<a class="link" target="_blank" rel="noopener" href="https://pytorch.org/serve/torchserve_on_wsl.html">文档<i class="fas fa-external-link-alt"></i></a>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/pytorch/serve.git</span><br><span class="line"><span class="built_in">cd</span> serve</span><br><span class="line"></span><br><span class="line">./ts_scripts/setup_wsl_ubuntu</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HOME</span>/.local/bin:<span class="variable">$PATH</span></span><br><span class="line">python ./ts_scripts/install_from_src.py</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;export PATH=$HOME/.local/bin:$PATH&#x27;</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="模型打包"><a href="#模型打包" class="headerlink" title="模型打包"></a>模型打包</h2><p>TorchServe 的一个关键特性是能够将所有模型工件打包到单个模型存档文件中。它是一个单独的命令行界面 (CLI)，torch-model-archiver，可以使用 state_dict 获取模型检查点或模型定义文件，并将它们打包成 .mar 文件。 然后，任何使用 TorchServe 的人都可以重新分发和提供该文件。它包含以下模型工件：在 torchscript 或模型定义文件的情况下的模型检查点文件和在急切模式的情况下的 state_dict 文件，以及服务模型可能需要的其他可选资产。 CLI 创建一个 .mar 文件，TorchServe 的服务器 CLI 使用该文件为模型提供服务。</p>
<p>torch-model-archiver 命令来打包模型，需要提供以下三个文件。</p>
<p>第 1 步：创建一个新的模型架构文件，其中包含从 torch.nn.modules 扩展的模型类。在这个例子中，我们创建了mnist模型文件mnist_model.py文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MnistClassificationNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.max_pool2d = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.dropout = nn.Dropout2d(p=<span class="number">0.25</span>)</span><br><span class="line">        <span class="comment"># self.relu = nn.ReLU()</span></span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">9216</span>, out_features=<span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">128</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x= self.max_pool2d(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">9216</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x= self.log_softmax(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>第 2 步：使用 <a href="https://yqstar.github.io/2022/05/11/Pytorch系列自学教程-2-深度学习“Hello-World”之Mnist图像分类/">mnist_sd</a> 训练 MNIST 数字识别模型并保存模型的状态字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;./checkpoints/model_pth/mnist_sd.pt&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>第 3 步：编写自定义处理程序以在您的模型上运行推理。 在此示例中，我们添加了一个 mnist_handler.py 文件，它使用上述模型对输入灰度图像进行推理并识别图像中的数字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> ts.torch_handler.image_classifier <span class="keyword">import</span> ImageClassifier</span><br><span class="line"><span class="keyword">from</span> torch.profiler <span class="keyword">import</span> ProfilerActivity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MNISTDigitClassifier</span>(<span class="title class_ inherited__">ImageClassifier</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MNISTDigitClassifier handler class. This handler extends class ImageClassifier from image_classifier.py, a</span></span><br><span class="line"><span class="string">    default handler. This handler takes an image and returns the number in that image.</span></span><br><span class="line"><span class="string">    Here method postprocess() has been overridden while others are reused from parent class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    image_processing = transforms.Compose([</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">        <span class="comment"># transforms.Normalize((0.1307,), (0.3081,))</span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MNISTDigitClassifier, self).__init__()</span><br><span class="line">        self.profiler_args = &#123;</span><br><span class="line">            <span class="string">&quot;activities&quot;</span> : [ProfilerActivity.CPU],</span><br><span class="line">            <span class="string">&quot;record_shapes&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The post process of MNIST converts the predicted output response to a label.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            data (list): The predicted output from the Inference with probabilities is passed</span></span><br><span class="line"><span class="string">            to the post-process function</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            list : A list of dictionaries with predictions and explanations is returned</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> data.argmax(<span class="number">1</span>).tolist()</span><br><span class="line">        <span class="comment"># return data.tolist()</span></span><br></pre></td></tr></table></figure>
<p>第 4 步：使用 torch-model-archiver 程序创建一个 Torch 模型存档以存档上述文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">$ torch-model-archiver -h</span><br><span class="line">usage: torch-model-archiver [-h] --model-name MODEL_NAME  --version MODEL_VERSION_NUMBER</span><br><span class="line">                      --model-file MODEL_FILE_PATH --serialized-file MODEL_SERIALIZED_PATH</span><br><span class="line">                      --handler HANDLER [--runtime &#123;python,python2,python3&#125;]</span><br><span class="line">                      [--export-path EXPORT_PATH] [-f] [--requirements-file]</span><br><span class="line"></span><br><span class="line">Model Archiver Tool</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  --model-name MODEL_NAME</span><br><span class="line">                        Exported model name. Exported file will be named as</span><br><span class="line">                        model-name.mar and saved <span class="keyword">in</span> current working directory</span><br><span class="line">                        <span class="keyword">if</span> no --export-path is specified, <span class="keyword">else</span> it will be</span><br><span class="line">                        saved under the <span class="built_in">export</span> path</span><br><span class="line">  --serialized-file SERIALIZED_FILE</span><br><span class="line">                        Path to .pt or .pth file containing state_dict <span class="keyword">in</span></span><br><span class="line">                        <span class="keyword">case</span> of eager mode or an executable ScriptModule</span><br><span class="line">                        <span class="keyword">in</span> <span class="keyword">case</span> of TorchScript.</span><br><span class="line">  --model-file MODEL_FILE</span><br><span class="line">                        Path to python file containing model architecture.</span><br><span class="line">                        This parameter is mandatory <span class="keyword">for</span> eager mode models.</span><br><span class="line">                        The model architecture file must contain only one</span><br><span class="line">                        class definition extended from torch.nn.modules.</span><br><span class="line">  --handler HANDLER     TorchServe<span class="string">&#x27;s default handler name  or handler python</span></span><br><span class="line"><span class="string">                        file path to handle custom TorchServe inference logic.</span></span><br><span class="line"><span class="string">  --extra-files EXTRA_FILES</span></span><br><span class="line"><span class="string">                        Comma separated path to extra dependency files.</span></span><br><span class="line"><span class="string">  --runtime &#123;python,python2,python3&#125;</span></span><br><span class="line"><span class="string">                        The runtime specifies which language to run your</span></span><br><span class="line"><span class="string">                        inference code on. The default runtime is</span></span><br><span class="line"><span class="string">                        RuntimeType.PYTHON. At the present moment we support</span></span><br><span class="line"><span class="string">                        the following runtimes python, python2, python3</span></span><br><span class="line"><span class="string">  --export-path EXPORT_PATH</span></span><br><span class="line"><span class="string">                        Path where the exported .mar file will be saved. This</span></span><br><span class="line"><span class="string">                        is an optional parameter. If --export-path is not</span></span><br><span class="line"><span class="string">                        specified, the file will be saved in the current</span></span><br><span class="line"><span class="string">                        working directory.</span></span><br><span class="line"><span class="string">  --archive-format &#123;tgz,default&#125;</span></span><br><span class="line"><span class="string">                        The format in which the model artifacts are archived.</span></span><br><span class="line"><span class="string">                        &quot;tgz&quot;: This creates the model-archive in &lt;model-name&gt;.tar.gz format.</span></span><br><span class="line"><span class="string">                        If platform hosting requires model-artifacts to be in &quot;.tar.gz&quot;</span></span><br><span class="line"><span class="string">                        use this option.</span></span><br><span class="line"><span class="string">                        &quot;no-archive&quot;: This option creates an non-archived version of model artifacts</span></span><br><span class="line"><span class="string">                        at &quot;export-path/&#123;model-name&#125;&quot; location. As a result of this choice,</span></span><br><span class="line"><span class="string">                        MANIFEST file will be created at &quot;export-path/&#123;model-name&#125;&quot; location</span></span><br><span class="line"><span class="string">                        without archiving these model files</span></span><br><span class="line"><span class="string">                        &quot;default&quot;: This creates the model-archive in &lt;model-name&gt;.mar format.</span></span><br><span class="line"><span class="string">                        This is the default archiving format. Models archived in this format</span></span><br><span class="line"><span class="string">                        will be readily hostable on TorchServe.</span></span><br><span class="line"><span class="string">  -f, --force           When the -f or --force flag is specified, an existing</span></span><br><span class="line"><span class="string">                        .mar file with same name as that provided in --model-</span></span><br><span class="line"><span class="string">                        name in the path specified by --export-path will</span></span><br><span class="line"><span class="string">                        overwritten</span></span><br><span class="line"><span class="string">  -v, --version         Model&#x27;</span>s version.</span><br><span class="line">  -r, --requirements-file</span><br><span class="line">                        Path to requirements.txt file containing a list of model specific python</span><br><span class="line">                        packages to be installed by TorchServe <span class="keyword">for</span> seamless model serving.</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch-model-archiver --model-name mnist --version 1.0 --model-file mnist_model.py --serialized-file mnist_sd.pt --export-path ./model_store --handler mnist_handler.py -f</span><br></pre></td></tr></table></figure>
<h2 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ torchserve --<span class="built_in">help</span></span><br><span class="line">usage: torchserve [-h] [-v | --version]</span><br><span class="line">                          [--start]</span><br><span class="line">                          [--stop]</span><br><span class="line">                          [--ts-config TS_CONFIG]</span><br><span class="line">                          [--model-store MODEL_STORE]</span><br><span class="line">                          [--workflow-store WORKFLOW_STORE]</span><br><span class="line">                          [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]</span><br><span class="line">                          [--log-config LOG_CONFIG]</span><br><span class="line"></span><br><span class="line">torchserve</span><br><span class="line"></span><br><span class="line">mandatory arguments:</span><br><span class="line">  --model-store MODEL_STORE</span><br><span class="line">                        Model store location <span class="built_in">where</span> models can be loaded</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  -v, --version         Return TorchServe Version</span><br><span class="line">  --start               Start the model-server</span><br><span class="line">  --stop                Stop the model-server</span><br><span class="line">  --ts-config TS_CONFIG</span><br><span class="line">                        Configuration file <span class="keyword">for</span> TorchServe</span><br><span class="line"></span><br><span class="line">  --models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]</span><br><span class="line">                        Models to be loaded using [model_name=]model_location</span><br><span class="line">                        format. Location can be a HTTP URL, a model archive</span><br><span class="line">                        file or directory contains model archive files <span class="keyword">in</span></span><br><span class="line">                        MODEL_STORE.</span><br><span class="line">  --log-config LOG_CONFIG</span><br><span class="line">                        Log4j configuration file <span class="keyword">for</span> TorchServe</span><br><span class="line">  --ncs, --no-config-snapshots         </span><br><span class="line">                        Disable snapshot feature</span><br><span class="line">  --workflow-store WORKFLOW_STORE</span><br><span class="line">                        Workflow store location <span class="built_in">where</span> workflow can be loaded. Defaults to model-store</span><br></pre></td></tr></table></figure>
<h3 id="启动torchserve服务"><a href="#启动torchserve服务" class="headerlink" title="启动torchserve服务"></a>启动torchserve服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchserve --start --ncs --model-store model_store --models mnist.mar</span><br></pre></td></tr></table></figure>
<p>模型启动日志如下截图：</p>
<p><img src="/2022/06/06/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-3-Mnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBTorchServe%E9%83%A8%E7%BD%B2/ts_start.png" alt="ts_start"></p>
<h3 id="推理健康检查API"><a href="#推理健康检查API" class="headerlink" title="推理健康检查API"></a>推理健康检查API</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8080/ping</span><br></pre></td></tr></table></figure>
<p>如果server正常运行, 响应会如截图所示：</p>
<p><img src="/2022/06/06/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-3-Mnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBTorchServe%E9%83%A8%E7%BD%B2/ts_ping.png" alt="ts_ping"></p>
<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://127.0.0.1:8080/predictions/mnist -T ./data/test.png</span><br></pre></td></tr></table></figure>
<p>test.png为数字为0的图片，通过上述的调用推理，可以看出结果是能正常返回的，是可以作为下游应用调用。</p>
<p><img src="/2022/06/06/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-3-Mnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBTorchServe%E9%83%A8%E7%BD%B2/ts_infer.png" alt="ts_infer"></p>
<h3 id="停止torchserve服务"><a href="#停止torchserve服务" class="headerlink" title="停止torchserve服务"></a>停止torchserve服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchserve --start</span><br></pre></td></tr></table></figure>
<h2 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h2><p><a class="link" target="_blank" rel="noopener" href="https://github.com/yqstar/Awesome_Pytorch_Tutorial/tree/master/Pytorch_Lesson3">完整代码<i class="fas fa-external-link-alt"></i></a>已上传Github，有需要的可以自行下载代码，如果对你有帮助，请Star，哈哈哈哈！</p>
<p>到此为止，已经可以使用自己数据玩耍各种Demo，快（苦）乐（逼）地进行炼丹之路。道路阻且长，行则将至，但行好事莫问前程。</p>
<ul>
<li>除了使用TorchServe部署模型，还有其他的解决方案吗？</li>
<li>除了使用提供这种Web API的形式，是否可以构建一个GUI的形式提供呢？例如 PYQT5 ？这里放一张PYQT5的图，后面会填坑。</li>
</ul>
<p><img src="/2022/06/06/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-3-Mnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BBTorchServe%E9%83%A8%E7%BD%B2/pyqt_demo.png" alt="pyqt_demo"></p>
<p>正如，人往往会对未知的事情产生恐惧，因为结局是未知的。所以当一切不再未知的时候，那么是不是就不会产生恐惧呢？</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>More info: <a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/344364948">Chenglu：如何部署PyTorch模型<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://github.com/pytorch/serve">TorchServe<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://github.com/pytorch/serve/tree/master/examples/image_classifier/mnist">TorchServe_Mnist Example<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41977938/article/details/122258595">随便写点笔记<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/Chris_zhangrx/article/details/117380516">PyTorch Eager mode and Script mode<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://cceyda.github.io/blog/huggingface/torchserve/streamlit/ner/2020/10/09/huggingface_streamlit_serve.html">Self-host your 🤗HuggingFace Transformer NER model with Torchserve + Streamlit<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://ceshiren.com/t/topic/20770">TorchServe搭建codeBERT分类模型服务<i class="fas fa-external-link-alt"></i></a><br>More info: <a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_15821487/article/details/122684773">torchserver模型本地部署和docker部署<i class="fas fa-external-link-alt"></i></a></p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li>本文标题：Pytorch深度学习教程(3):Mnist图像分类TorchServe部署</li>
        <li>本文作者：AiLab123</li>
        <li>创建时间：2022-06-06 18:05:23</li>
        <li>
            本文链接：https://yqstar.github.io/2022/06/06/Pytorch深度学习教程-3-Mnist图像分类TorchServe部署/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Pytorch-Tutorial/">#Pytorch_Tutorial</a>&nbsp;
                    </li>
                
            </ul>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2022/06/12/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-4-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Pytorch深度学习教程(4):序列模型</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/05/29/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B-2-%E2%80%9CHello-World%E2%80%9D%E4%B9%8BMnist%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Pytorch深度学习教程(2):“Hello World”之Mnist图像分类</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2022&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">AiLab123</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TorchServe%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">TorchServe简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="nav-number">3.</span> <span class="nav-text">环境安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Conda%E9%85%8D%E7%BD%AE"><span class="nav-number">3.1.</span> <span class="nav-text">Conda配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TS%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85"><span class="nav-number">3.2.</span> <span class="nav-text">TS源码安装</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85"><span class="nav-number">4.</span> <span class="nav-text">模型打包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="nav-number">5.</span> <span class="nav-text">模型部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8torchserve%E6%9C%8D%E5%8A%A1"><span class="nav-number">5.1.</span> <span class="nav-text">启动torchserve服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5API"><span class="nav-number">5.2.</span> <span class="nav-text">推理健康检查API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86"><span class="nav-number">5.3.</span> <span class="nav-text">推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2torchserve%E6%9C%8D%E5%8A%A1"><span class="nav-number">5.4.</span> <span class="nav-text">停止torchserve服务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2"><span class="nav-number">6.</span> <span class="nav-text">探索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>



<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/dark-light-toggle.js"></script>


    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/local-search.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/code-copy.js"></script>




<div class="post-scripts pjax">
    
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/left-side-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/libs/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/toc.js"></script>
    
</div>


    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.4.5/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
