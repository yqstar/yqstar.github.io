---
title: Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹(3):æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²
date: 2022-05-15 18:05:23
tags: Pytorch_Tutorial
categories:
---

## éƒ¨ç½²ç®€ä»‹

å­¦ä¹ åº”è¯¥èƒ½å¤Ÿç»ˆç«¯è½åœ°ï¼Œè¿™æ ·å¯¹äºä¸ªäººå¸¦æ¥çš„æ»¡è¶³æ‰æ˜¯å·¨å¤§çš„ï¼Œæ‰ä¼šæ˜¯æœ‰ä»·å€¼çš„ã€‚æ‰€ä»¥å•çº¯åœ°è·‘äº†Demoæ¨¡å‹ï¼Œå¯¹è‡ªå·±çš„æŠ€æœ¯æå‡è¿˜æ˜¯å¾ˆå¤§çš„ï¼Œä½†æ˜¯å¦‚ä½•æŠŠæŠ€æœ¯è¾“å‡ºä¹Ÿæ˜¯å¿…è¦çš„ã€‚

é‚£ä¹ˆä½¿ç”¨ *PyTorch* è®­ç»ƒå¥½ Demo æ¨¡å‹ï¼Œå¦‚ä½•éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæä¾›ç”¨äºæä¾›æ¨¡å‹æœåŠ¡å‘¢ï¼Ÿéƒ¨ç½²å½¢å¼æ˜¯éå¸¸å¤šæ ·çš„ï¼Œæœ¬ç« å†…å®¹ä¸»è¦ä»‹ç»çš„æ˜¯æ˜¯å¤§å‚å‡ºå“çš„TorchServeç”¨äºéƒ¨ç½²Pytorchçš„æ¨¡å‹ã€‚

æ¨¡å‹æœåŠ¡æ˜¯åœ¨ç³»ç»Ÿä¸­æ”¾ç½®ç»è¿‡è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿‡ç¨‹ï¼Œä»¥ä¾¿å®ƒå¯ä»¥æ¥å—æ–°çš„è¾“å…¥å¹¶å°†æ¨ç†è¿”å›ç»™ç³»ç»Ÿã€‚

## TorchServeç®€ä»‹

Torchserve æ˜¯ PyTorch çš„é¦–é€‰æ¨¡å‹æœåŠ¡è§£å†³æ–¹æ¡ˆã€‚å®ƒå…è®¸æ‚¨ä¸ºæ‚¨çš„æ¨¡å‹å…¬å¼€ä¸€ä¸ª Web APIï¼Œå¯ä»¥ç›´æ¥è®¿é—®æˆ–é€šè¿‡æ‚¨çš„åº”ç”¨ç¨‹åºè®¿é—®ã€‚

> TorchServe is a performant, flexible and easy to use tool for serving PyTorch eager mode and torschripted models.

ä»ä¸Šé¢çš„å®˜ç½‘ä»‹ç»çš„å†…å®¹å¯ä»¥çœ‹å‡ºï¼šTorchServeçš„ç‰¹ç‚¹æ˜¯æ€§èƒ½å¥½ã€çµæ´»æ€§å¥½ã€æ˜“ä½¿ç”¨çš„å·¥å…·ï¼Œå…¶æ¬¡é¢å‘éƒ¨ç½²çš„æ¨¡å‹æ˜¯Pytorchçš„Eageræ¨¡å¼å’ŒScriptæ¨¡å¼çš„æ¨¡å‹ã€‚

TorchServeæ˜¯ç”±AWSå’ŒFacebookåˆä½œå¼€å‘çš„PyTorchæ¨¡å‹æœåŠ¡åº“ï¼Œæ˜¯ [PyTorchå¼€æºé¡¹ç›®](https://pytorch.org/serve/index.html) éƒ¨åˆ†ã€‚

å€ŸåŠ©TorchServeï¼ŒPyTorchç”¨æˆ·å¯ä»¥æ›´å¿«åœ°å°†å…¶æ¨¡å‹åº”ç”¨äºç”Ÿäº§ï¼Œè€Œæ— éœ€ç¼–å†™è‡ªå®šä¹‰ä»£ç ï¼šé™¤äº†æä¾›ä½å»¶è¿Ÿé¢„æµ‹APIä¹‹å¤–ï¼ŒTorchServeè¿˜ä¸ºä¸€äº›æœ€å¸¸è§çš„åº”ç”¨ç¨‹åºåµŒå…¥äº†é»˜è®¤å¤„ç†ç¨‹åºï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹å’Œæ–‡æœ¬åˆ†ç±»ã€‚æ­¤å¤–ï¼ŒTorchServeåŒ…æ‹¬å¤šæ¨¡å‹æœåŠ¡ã€ç”¨äºA/B æµ‹è¯•çš„æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶ã€ç›‘è§†æŒ‡æ ‡ä»¥åŠç”¨äºåº”ç”¨ç¨‹åºé›†æˆçš„RESTfulç«¯ç‚¹ã€‚å¦‚ä½ æ‰€æ–™ï¼ŒTorchServeæ”¯æŒä»»ä½•æœºå™¨å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬Amazon SageMakerã€å®¹å™¨æœåŠ¡å’ŒAmazon Elastic Compute Cloudã€‚

TorchServeæ¡†æ¶ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼šFrontendæ˜¯TorchServeçš„è¯·æ±‚å’Œå“åº”çš„å¤„ç†éƒ¨åˆ†ï¼›Worker Process æŒ‡çš„æ˜¯ä¸€ç»„è¿è¡Œçš„æ¨¡å‹å®ä¾‹ï¼Œå¯ä»¥ç”±ç®¡ç†APIè®¾å®šè¿è¡Œçš„æ•°é‡ï¼›Model Storeæ˜¯æ¨¡å‹å­˜å‚¨åŠ è½½çš„åœ°æ–¹ï¼›Backendç”¨äºç®¡ç†Worker Processã€‚

![ts_frame](../_posts/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_frame.png)

## ç¯å¢ƒå®‰è£…

æˆ‘è¿™è¾¹ä½¿ç”¨çš„æ˜¯Windows11ï¼ŒWSL2ç¯å¢ƒç³»ç»Ÿè¿›è¡Œéƒ¨ç½²ä½œä¸šã€‚

* å®‰è£… JDK11
  
``` bash
sudo apt-get install openjdk-11-jdk
```

* å®‰è£…ä¾èµ–

``` bash
pip install torch torchtext torchvision sentencepiece psutil future
pip install torchserve torch-model-archiver
```

TorchServeæºå®‰è£…

``` bash
git clone https://github.com/pytorch/serve.git
cd serve

./ts_scripts/setup_wsl_ubuntu
export PATH=$HOME/.local/bin:$PATH
python ./ts_scripts/install_from_src.py
echo 'export PATH=$HOME/.local/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
```

## æ¨¡å‹æ‰“åŒ…

TorchServe çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯èƒ½å¤Ÿå°†æ‰€æœ‰æ¨¡å‹å·¥ä»¶æ‰“åŒ…åˆ°å•ä¸ªæ¨¡å‹å­˜æ¡£æ–‡ä»¶ä¸­ã€‚å®ƒæ˜¯ä¸€ä¸ªå•ç‹¬çš„å‘½ä»¤è¡Œç•Œé¢ (CLI)ï¼Œtorch-model-archiverï¼Œå¯ä»¥ä½¿ç”¨ state_dict è·å–æ¨¡å‹æ£€æŸ¥ç‚¹æˆ–æ¨¡å‹å®šä¹‰æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬æ‰“åŒ…æˆ .mar æ–‡ä»¶ã€‚ ç„¶åï¼Œä»»ä½•ä½¿ç”¨ TorchServe çš„äººéƒ½å¯ä»¥é‡æ–°åˆ†å‘å’Œæä¾›è¯¥æ–‡ä»¶ã€‚å®ƒåŒ…å«ä»¥ä¸‹æ¨¡å‹å·¥ä»¶ï¼šåœ¨ torchscript æˆ–æ¨¡å‹å®šä¹‰æ–‡ä»¶çš„æƒ…å†µä¸‹çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶å’Œåœ¨æ€¥åˆ‡æ¨¡å¼çš„æƒ…å†µä¸‹çš„ state_dict æ–‡ä»¶ï¼Œä»¥åŠæœåŠ¡æ¨¡å‹å¯èƒ½éœ€è¦çš„å…¶ä»–å¯é€‰èµ„äº§ã€‚ CLI åˆ›å»ºä¸€ä¸ª .mar æ–‡ä»¶ï¼ŒTorchServe çš„æœåŠ¡å™¨ CLI ä½¿ç”¨è¯¥æ–‡ä»¶ä¸ºæ¨¡å‹æä¾›æœåŠ¡ã€‚

torch-model-archiver å‘½ä»¤æ¥æ‰“åŒ…æ¨¡å‹ï¼Œéœ€è¦æä¾›ä»¥ä¸‹ä¸‰ä¸ªæ–‡ä»¶ã€‚

ç¬¬ 1 æ­¥ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹æ¶æ„æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä» torch.nn.modules æ‰©å±•çš„æ¨¡å‹ç±»ã€‚ åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº† mnist æ¨¡å‹æ–‡ä»¶ mnist_model.py æ–‡ä»¶ã€‚

``` python
import torch
from torch import nn
import torch.nn.functional as F

# æ„å»ºç½‘ç»œ
class MnistClassificationNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
        self.max_pool2d = nn.MaxPool2d(kernel_size=2)
        self.dropout = nn.Dropout2d(p=0.25)
        # self.relu = nn.ReLU()
        self.fc1 = nn.Linear(in_features=9216, out_features=128)
        self.fc2 = nn.Linear(in_features=128, out_features=10)
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x= self.max_pool2d(x)
        x = self.dropout(x)
        x = x.view(-1, 9216)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x= self.log_softmax(x)
        return x
```

ç¬¬ 2 æ­¥ï¼šä½¿ç”¨ [mnist_sd](https://yqstar.github.io/2022/05/11/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-2-æ·±åº¦å­¦ä¹ â€œHello-Worldâ€ä¹‹Mnistå›¾åƒåˆ†ç±»/) è®­ç»ƒ MNIST æ•°å­—è¯†åˆ«æ¨¡å‹å¹¶ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚

``` python
torch.save(model.state_dict(), "./checkpoints/model_pth/mnist_sd.pt")
```

ç¬¬ 3 æ­¥ï¼šç¼–å†™è‡ªå®šä¹‰å¤„ç†ç¨‹åºä»¥åœ¨æ‚¨çš„æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚ åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª mnist_handler.py æ–‡ä»¶ï¼Œå®ƒä½¿ç”¨ä¸Šè¿°æ¨¡å‹å¯¹è¾“å…¥ç°åº¦å›¾åƒè¿›è¡Œæ¨ç†å¹¶è¯†åˆ«å›¾åƒä¸­çš„æ•°å­—ã€‚

``` python
from torchvision import transforms
from ts.torch_handler.image_classifier import ImageClassifier
from torch.profiler import ProfilerActivity


class MNISTDigitClassifier(ImageClassifier):
    """
    MNISTDigitClassifier handler class. This handler extends class ImageClassifier from image_classifier.py, a
    default handler. This handler takes an image and returns the number in that image.
    Here method postprocess() has been overridden while others are reused from parent class.
    """

    image_processing = transforms.Compose([
        transforms.ToTensor()
        # transforms.Normalize((0.1307,), (0.3081,))
    ])

    def __init__(self):
        super(MNISTDigitClassifier, self).__init__()
        self.profiler_args = {
            "activities" : [ProfilerActivity.CPU],
            "record_shapes": True,
        }


    def postprocess(self, data):
        """The post process of MNIST converts the predicted output response to a label.
        Args:
            data (list): The predicted output from the Inference with probabilities is passed
            to the post-process function
        Returns:
            list : A list of dictionaries with predictions and explanations is returned
        """
        return data.argmax(1).tolist()
        # return data.tolist()
```

ç¬¬ 4 æ­¥ï¼šä½¿ç”¨ torch-model-archiver ç¨‹åºåˆ›å»ºä¸€ä¸ª Torch æ¨¡å‹å­˜æ¡£ä»¥å­˜æ¡£ä¸Šè¿°æ–‡ä»¶ã€‚

``` bash
$ torch-model-archiver -h
usage: torch-model-archiver [-h] --model-name MODEL_NAME  --version MODEL_VERSION_NUMBER
                      --model-file MODEL_FILE_PATH --serialized-file MODEL_SERIALIZED_PATH
                      --handler HANDLER [--runtime {python,python2,python3}]
                      [--export-path EXPORT_PATH] [-f] [--requirements-file]

Model Archiver Tool

optional arguments:
  -h, --help            show this help message and exit
  --model-name MODEL_NAME
                        Exported model name. Exported file will be named as
                        model-name.mar and saved in current working directory
                        if no --export-path is specified, else it will be
                        saved under the export path
  --serialized-file SERIALIZED_FILE
                        Path to .pt or .pth file containing state_dict in
                        case of eager mode or an executable ScriptModule
                        in case of TorchScript.
  --model-file MODEL_FILE
                        Path to python file containing model architecture.
                        This parameter is mandatory for eager mode models.
                        The model architecture file must contain only one
                        class definition extended from torch.nn.modules.
  --handler HANDLER     TorchServe's default handler name  or handler python
                        file path to handle custom TorchServe inference logic.
  --extra-files EXTRA_FILES
                        Comma separated path to extra dependency files.
  --runtime {python,python2,python3}
                        The runtime specifies which language to run your
                        inference code on. The default runtime is
                        RuntimeType.PYTHON. At the present moment we support
                        the following runtimes python, python2, python3
  --export-path EXPORT_PATH
                        Path where the exported .mar file will be saved. This
                        is an optional parameter. If --export-path is not
                        specified, the file will be saved in the current
                        working directory.
  --archive-format {tgz,default}
                        The format in which the model artifacts are archived.
                        "tgz": This creates the model-archive in <model-name>.tar.gz format.
                        If platform hosting requires model-artifacts to be in ".tar.gz"
                        use this option.
                        "no-archive": This option creates an non-archived version of model artifacts
                        at "export-path/{model-name}" location. As a result of this choice,
                        MANIFEST file will be created at "export-path/{model-name}" location
                        without archiving these model files
                        "default": This creates the model-archive in <model-name>.mar format.
                        This is the default archiving format. Models archived in this format
                        will be readily hostable on TorchServe.
  -f, --force           When the -f or --force flag is specified, an existing
                        .mar file with same name as that provided in --model-
                        name in the path specified by --export-path will
                        overwritten
  -v, --version         Model's version.
  -r, --requirements-file
                        Path to requirements.txt file containing a list of model specific python
                        packages to be installed by TorchServe for seamless model serving.
```

``` bash
torch-model-archiver --model-name mnist --version 1.0 --model-file mnist_model.py --serialized-file mnist_sd.pt --export-path ./model_store --handler mnist_handler.py -f
```

## æ¨¡å‹éƒ¨ç½²

``` bash
$ torchserve --help
usage: torchserve [-h] [-v | --version]
                          [--start]
                          [--stop]
                          [--ts-config TS_CONFIG]
                          [--model-store MODEL_STORE]
                          [--workflow-store WORKFLOW_STORE]
                          [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]
                          [--log-config LOG_CONFIG]

torchserve

mandatory arguments:
  --model-store MODEL_STORE
                        Model store location where models can be loaded

  

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         Return TorchServe Version
  --start               Start the model-server
  --stop                Stop the model-server
  --ts-config TS_CONFIG
                        Configuration file for TorchServe

  --models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]
                        Models to be loaded using [model_name=]model_location
                        format. Location can be a HTTP URL, a model archive
                        file or directory contains model archive files in
                        MODEL_STORE.
  --log-config LOG_CONFIG
                        Log4j configuration file for TorchServe
  --ncs, --no-config-snapshots         
                        Disable snapshot feature
  --workflow-store WORKFLOW_STORE
                        Workflow store location where workflow can be loaded. Defaults to model-store
```

### å¯åŠ¨torchserveæœåŠ¡

``` bash
torchserve --start --ncs --model-store model_store --models mnist.mar
```

æ¨¡å‹å¯åŠ¨æ—¥å¿—å¦‚ä¸‹æˆªå›¾ï¼š

![ts_start](../_posts/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_start.png)

### æ¨ç†å¥åº·æ£€æŸ¥API

``` bash
curl http://localhost:8080/ping
```

å¦‚æœserveræ­£å¸¸è¿è¡Œ, å“åº”ä¼šå¦‚æˆªå›¾æ‰€ç¤ºï¼š

![ts_ping](../_posts/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_ping.png)

### æ¨ç†

``` bash
curl http://127.0.0.1:8080/predictions/mnist -T ./data/test.png
```

test.pngä¸ºæ•°å­—ä¸º0çš„å›¾ç‰‡ï¼Œé€šè¿‡ä¸Šè¿°çš„è°ƒç”¨æ¨ç†ï¼Œå¯ä»¥çœ‹å‡ºç»“æœæ˜¯èƒ½æ­£å¸¸è¿”å›çš„ï¼Œæ˜¯å¯ä»¥ä½œä¸ºä¸‹æ¸¸åº”ç”¨è°ƒç”¨ã€‚

![ts_infer](../_posts/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_infer.png)

### åœæ­¢torchserveæœåŠ¡

``` bash
torchserve --start
```

## æ¢ç´¢

[å®Œæ•´ä»£ç ](https://github.com/yqstar/Awesome_Pytorch_Tutorial/tree/master/Pytorch_Lesson3)å·²ä¸Šä¼ Githubï¼Œæœ‰éœ€è¦çš„å¯ä»¥è‡ªè¡Œä¸‹è½½ä»£ç ï¼Œå¦‚æœå¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·Starï¼Œå“ˆå“ˆå“ˆå“ˆï¼

åˆ°æ­¤ä¸ºæ­¢ï¼Œå·²ç»å¯ä»¥ä½¿ç”¨è‡ªå·±æ•°æ®ç©è€å„ç§Demoï¼Œå¿«ï¼ˆè‹¦ï¼‰ä¹ï¼ˆé€¼ï¼‰åœ°è¿›è¡Œç‚¼ä¸¹ä¹‹è·¯ã€‚é“è·¯é˜»ä¸”é•¿ï¼Œè¡Œåˆ™å°†è‡³ï¼Œä½†è¡Œå¥½äº‹è«é—®å‰ç¨‹ã€‚

* é™¤äº†ä½¿ç”¨TorchServeéƒ¨ç½²æ¨¡å‹ï¼Œè¿˜æœ‰å…¶ä»–çš„è§£å†³æ–¹æ¡ˆå—ï¼Ÿ
* é™¤äº†ä½¿ç”¨æä¾›è¿™ç§Web APIçš„å½¢å¼ï¼Œæ˜¯å¦å¯ä»¥æ„å»ºä¸€ä¸ªGUIçš„å½¢å¼æä¾›å‘¢ï¼Ÿä¾‹å¦‚ PYQT5 ï¼Ÿè¿™é‡Œæ”¾ä¸€å¼ PYQT5çš„å›¾ï¼Œåé¢ä¼šå¡«å‘ã€‚

![pyqt_demo](../_posts/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/pyqt_demo.png)

æ­£å¦‚ï¼Œäººå¾€å¾€ä¼šå¯¹æœªçŸ¥çš„äº‹æƒ…äº§ç”Ÿææƒ§ï¼Œå› ä¸ºç»“å±€æ˜¯æœªçŸ¥çš„ã€‚æ‰€ä»¥å½“ä¸€åˆ‡ä¸å†æœªçŸ¥çš„æ—¶å€™ï¼Œé‚£ä¹ˆæ˜¯ä¸æ˜¯å°±ä¸ä¼šäº§ç”Ÿææƒ§å‘¢ï¼Ÿ

## å‚è€ƒ

More info: [Chengluï¼šå¦‚ä½•éƒ¨ç½²PyTorchæ¨¡å‹](https://zhuanlan.zhihu.com/p/344364948)
More info: [TorchServe](https://github.com/pytorch/serve)
More info: [TorchServe_Mnist Example](https://github.com/pytorch/serve/tree/master/examples/image_classifier/mnist)
More info: [éšä¾¿å†™ç‚¹ç¬”è®°](https://blog.csdn.net/weixin_41977938/article/details/122258595)
More info: [PyTorch Eager mode and Script mode](https://blog.csdn.net/Chris_zhangrx/article/details/117380516)
More info: [Self-host your ğŸ¤—HuggingFace Transformer NER model with Torchserve + Streamlit](https://cceyda.github.io/blog/huggingface/torchserve/streamlit/ner/2020/10/09/huggingface_streamlit_serve.html)
More info: [TorchServeæ­å»ºcodeBERTåˆ†ç±»æ¨¡å‹æœåŠ¡](https://ceshiren.com/t/topic/20770)
More info: [torchserveræ¨¡å‹æœ¬åœ°éƒ¨ç½²å’Œdockeréƒ¨ç½²](https://blog.csdn.net/qq_15821487/article/details/122684773)
