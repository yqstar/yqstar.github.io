---
title: Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹(3):æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²
date: 2022-06-06 18:05:23
tags: Pytorch_Tutorial
categories:
---

## ç®€ä»‹

ç»è¿‡å‰ [ä¸¤ç¯‡åšå®¢](https://yqstar.github.io/tags/Pytorch-Tutorial/) å­¦ä¹ ï¼Œæˆ‘ä»¬å·²å¯ä½¿ç”¨CNNæ¨¡å‹å®ŒæˆMnistæ‰‹å†™æ•°å­—åˆ†ç±»æ¨¡å‹ï¼Œå¯¹äºç®—æ³•ä»æ•°æ®å¤„ç†ã€æ¨¡å‹æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°é“¾è·¯æœ‰åˆæ­¥è®¤çŸ¥ã€‚ä½†å·¥ä¸šå¯èƒ½éœ€è¦éƒ¨ç½²ç¦»çº¿åœ¨çº¿æ¨¡å‹ç”¨äºæä¾›æ¨¡å‹æ¨ç†æœåŠ¡ï¼Œæ‰€è°“æ¨¡å‹æ¨ç†æœåŠ¡æ˜¯æŒ‡åœ¨ç³»ç»Ÿé…ç½®è®­ç»ƒå®Œæˆæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¾¿å…¶å¯æ¥å—æ–°çš„è¾“å…¥å¹¶å°†æ¨ç†ç»“æœè¿”å›ç»™ç³»ç»Ÿã€‚

å…¶æ¬¡ï¼Œè™½ç„¶å¾ˆå¤šå¤§å‚éƒ½ä¼šæœ‰å°è£…å¥½éƒ¨ç½²å¹³å°ä¾›ç®—æ³•äººå‘˜ä¾¿æ·é…ç½®ï¼Œä½†æ˜¯å­¦ä¹ ä¸­å¯¹äºå®Œæ•´çš„å·¥ç¨‹é“¾è·¯å¼€å‘å¯¹äºä¸ªäººèƒ½åŠ›å»ºè®¾ä¹Ÿæ˜¯éå¸¸é‡è¦çš„ï¼Œè€Œä¸æ˜¯ä»…ä»…ä½œä¸ºä¸€é¢—èºä¸é’‰ï¼Œå¦‚ä½•å®ç°ä»Demoæ¨¡å‹è½¬æ¢æˆçº¿ä¸Šæ¨¡å‹æ¨ç†æœåŠ¡éƒ¨ç½²ï¼Œå¯¹äºä¸ªäººçš„æ­£å‘åé¦ˆä¹Ÿæ˜¯éå¸¸æœ‰æ„ä¹‰çš„ã€‚

é‚£ä¹ˆé’ˆå¯¹ *PyTorch* è®­ç»ƒå¥½ Demo æ¨¡å‹ï¼Œå¦‚ä½•éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒç”¨äºæä¾›æ¨¡å‹æ¨ç†æœåŠ¡å‘¢ï¼Ÿéƒ¨ç½²å½¢å¼éå¸¸å¤šæ ·ï¼Œå…¶ä¸­ TorchServe æ˜¯ [PyTorchå¼€æºé¡¹ç›®](https://pytorch.org/serve/index.html) éƒ¨åˆ†ï¼Œæ˜¯AWSå’ŒFacebookåˆä½œå¼€å‘çš„ç”¨äºéƒ¨ç½²Pytorchçš„æ¨¡å‹ï¼Œå¯¹äºç®—æ³•å·¥ç¨‹å¸ˆæ˜¯ç›¸å½“å‹å¥½çš„ã€‚æœ¬ç« ä»‹ç»å¦‚ä½•ä½¿ç”¨TorchServeå®ŒæˆPyTorchæ¨¡å‹çš„éƒ¨ç½²å’Œè°ƒç”¨ã€‚

## TorchServeç®€ä»‹

Torchserveæ˜¯PyTorchçš„é¦–é€‰æ¨¡å‹éƒ¨ç½²è§£å†³æ–¹æ¡ˆã€‚å®ƒå…è®¸ä¸ºæ¨¡å‹å…¬å¼€ä¸€ä¸ªå¯ä¾›ç›´æ¥è®¿é—®æˆ–è€…åº”ç”¨ç¨‹åºè®¿é—®çš„WebAPIï¼Œå€ŸåŠ©TorchServeï¼ŒPyTorchç”¨æˆ·å¯ä»¥æ›´å¿«åœ°å°†å…¶æ¨¡å‹åº”ç”¨äºç”Ÿäº§ï¼Œè€Œæ— éœ€ç¼–å†™è‡ªå®šä¹‰ä»£ç ï¼Œæ­¤å¤–ï¼ŒTorchServeå°†å·¥ç¨‹å¼€å‘å’Œç®—æ³•å¼€å‘è¿›è¡Œè§£è€¦ï¼Œç®—æ³•å·¥ç¨‹å¸ˆä¸»è¦å®Œæˆæ•°æ®Processå’Œæ¨¡å‹æ„å»ºè¿™ä¸€æ“…é•¿é¢†åŸŸï¼Œå…¶ä»–çš„å¤šæ¨¡å‹æœåŠ¡ã€A/Bæµ‹è¯•çš„ç‰ˆæœ¬æ§åˆ¶ã€ç›‘è§†æŒ‡æ ‡ä»¥åŠåº”ç”¨ç¨‹åºé›†æˆRESTfuléƒ½å·²å°è£…å¥½ã€‚

> TorchServe is a performant, flexible and easy to use tool for serving PyTorch eager mode and torschripted models.

å®˜ç½‘ä»‹ç»å¯çœ‹å‡ºï¼šTorchServeæ˜¯ä¸€æ¬¾æ€§èƒ½å¥½ã€çµæ´»æ€§å¥½ã€æ˜“ä½¿ç”¨çš„å·¥å…·ï¼Œå…¶æ¬¡å¯éƒ¨ç½²æ¨¡å‹ç±»å‹æ˜¯Pytorchçš„Eageræ¨¡å¼å’ŒScriptæ¨¡å¼æ¨¡å‹ã€‚

TorchServeæ¡†æ¶ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼šFrontendæ˜¯TorchServeçš„è¯·æ±‚å’Œå“åº”çš„å¤„ç†éƒ¨åˆ†ï¼›WorkerProcess æŒ‡çš„æ˜¯ä¸€ç»„è¿è¡Œçš„æ¨¡å‹å®ä¾‹ï¼Œå¯ä»¥ç”±ç®¡ç†APIè®¾å®šè¿è¡Œçš„æ•°é‡ï¼›Model Storeæ˜¯æ¨¡å‹å­˜å‚¨åŠ è½½çš„åœ°æ–¹ï¼›Backendç”¨äºç®¡ç†Worker Processï¼Œå…·ä½“å¯å‚è€ƒä¸‹å›¾é‡Œã€‚

![ts_frame](Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_frame.png)

## ç¯å¢ƒå®‰è£…

æœ¬äººä½¿ç”¨ Windows11+WSL2+Ubuntu ç¯å¢ƒè¿›è¡Œéƒ¨ç½²ã€‚

### Condaé…ç½®

[å®˜ç½‘è¦æ±‚](https://github.com/pytorch/serve)ï¼šPython Version >= 3.8ï¼Œæœ¬æ–‡ä½¿ç”¨Condaç®¡ç†æ·±åº¦å­¦ä¹ ç¯å¢ƒï¼Œå…·ä½“ä½¿ç”¨å¯å‚è€ƒä¹‹å‰çš„åšæ–‡ï¼š[æ·±åº¦å­¦ä¹ ç®¡ç†é…ç½®](https://yqstar.github.io/2022/05/01/Windows%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8Conda%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/)ã€‚

![python_version](Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/python_version.png)

å¯ä½¿ç”¨ä¸‹è¿°å‘½ä»¤åˆ›å»ºCondaçš„Pythonç¯å¢ƒï¼ˆpythonç‰ˆæœ¬ä¸º3.8ï¼Œç¯å¢ƒåä¸ºts_ENVï¼‰å’Œæ¿€æ´»æŒ‡å®šç¯å¢ƒ(ts_env)ã€‚

``` bash
conda create --name ts_env python=3.8
conda activate ts_env
```

### TSæºç å®‰è£…

å¯å‚è€ƒå®˜ç½‘TSå®‰è£…[æ–‡æ¡£](https://pytorch.org/serve/torchserve_on_wsl.html)ã€‚

``` bash
git clone https://github.com/pytorch/serve.git
cd serve

./ts_scripts/setup_wsl_ubuntu
export PATH=$HOME/.local/bin:$PATH
python ./ts_scripts/install_from_src.py
echo 'export PATH=$HOME/.local/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
```

## æ¨¡å‹æ‰“åŒ…

TorchServe çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯èƒ½å¤Ÿå°†æ‰€æœ‰æ¨¡å‹å·¥ä»¶æ‰“åŒ…åˆ°å•ä¸ªæ¨¡å‹å­˜æ¡£æ–‡ä»¶ä¸­ã€‚å®ƒæ˜¯ä¸€ä¸ªå•ç‹¬çš„å‘½ä»¤è¡Œç•Œé¢ (CLI)ï¼Œtorch-model-archiverï¼Œå¯ä»¥ä½¿ç”¨ state_dict è·å–æ¨¡å‹æ£€æŸ¥ç‚¹æˆ–æ¨¡å‹å®šä¹‰æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬æ‰“åŒ…æˆ .mar æ–‡ä»¶ã€‚ ç„¶åï¼Œä»»ä½•ä½¿ç”¨ TorchServe çš„äººéƒ½å¯ä»¥é‡æ–°åˆ†å‘å’Œæä¾›è¯¥æ–‡ä»¶ã€‚å®ƒåŒ…å«ä»¥ä¸‹æ¨¡å‹å·¥ä»¶ï¼šåœ¨ torchscript æˆ–æ¨¡å‹å®šä¹‰æ–‡ä»¶çš„æƒ…å†µä¸‹çš„æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶å’Œåœ¨æ€¥åˆ‡æ¨¡å¼çš„æƒ…å†µä¸‹çš„ state_dict æ–‡ä»¶ï¼Œä»¥åŠæœåŠ¡æ¨¡å‹å¯èƒ½éœ€è¦çš„å…¶ä»–å¯é€‰èµ„äº§ã€‚ CLI åˆ›å»ºä¸€ä¸ª .mar æ–‡ä»¶ï¼ŒTorchServe çš„æœåŠ¡å™¨ CLI ä½¿ç”¨è¯¥æ–‡ä»¶ä¸ºæ¨¡å‹æä¾›æœåŠ¡ã€‚

torch-model-archiver å‘½ä»¤æ¥æ‰“åŒ…æ¨¡å‹ï¼Œéœ€è¦æä¾›ä»¥ä¸‹ä¸‰ä¸ªæ–‡ä»¶ã€‚

ç¬¬ 1 æ­¥ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹æ¶æ„æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä» torch.nn.modules æ‰©å±•çš„æ¨¡å‹ç±»ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†mnistæ¨¡å‹æ–‡ä»¶mnist_model.pyæ–‡ä»¶ã€‚

``` python
import torch
from torch import nn
import torch.nn.functional as F

# æ„å»ºç½‘ç»œ
class MnistClassificationNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
        self.max_pool2d = nn.MaxPool2d(kernel_size=2)
        self.dropout = nn.Dropout2d(p=0.25)
        # self.relu = nn.ReLU()
        self.fc1 = nn.Linear(in_features=9216, out_features=128)
        self.fc2 = nn.Linear(in_features=128, out_features=10)
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x= self.max_pool2d(x)
        x = self.dropout(x)
        x = x.view(-1, 9216)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x= self.log_softmax(x)
        return x
```

ç¬¬ 2 æ­¥ï¼šä½¿ç”¨ [mnist_sd](https://yqstar.github.io/2022/05/11/Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-2-æ·±åº¦å­¦ä¹ â€œHello-Worldâ€ä¹‹Mnistå›¾åƒåˆ†ç±»/) è®­ç»ƒ MNIST æ•°å­—è¯†åˆ«æ¨¡å‹å¹¶ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚

``` python
torch.save(model.state_dict(), "./checkpoints/model_pth/mnist_sd.pt")
```

ç¬¬ 3 æ­¥ï¼šç¼–å†™è‡ªå®šä¹‰å¤„ç†ç¨‹åºä»¥åœ¨æ‚¨çš„æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚ åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª mnist_handler.py æ–‡ä»¶ï¼Œå®ƒä½¿ç”¨ä¸Šè¿°æ¨¡å‹å¯¹è¾“å…¥ç°åº¦å›¾åƒè¿›è¡Œæ¨ç†å¹¶è¯†åˆ«å›¾åƒä¸­çš„æ•°å­—ã€‚

``` python
from torchvision import transforms
from ts.torch_handler.image_classifier import ImageClassifier
from torch.profiler import ProfilerActivity


class MNISTDigitClassifier(ImageClassifier):
    """
    MNISTDigitClassifier handler class. This handler extends class ImageClassifier from image_classifier.py, a
    default handler. This handler takes an image and returns the number in that image.
    Here method postprocess() has been overridden while others are reused from parent class.
    """

    image_processing = transforms.Compose([
        transforms.ToTensor()
        # transforms.Normalize((0.1307,), (0.3081,))
    ])

    def __init__(self):
        super(MNISTDigitClassifier, self).__init__()
        self.profiler_args = {
            "activities" : [ProfilerActivity.CPU],
            "record_shapes": True,
        }


    def postprocess(self, data):
        """The post process of MNIST converts the predicted output response to a label.
        Args:
            data (list): The predicted output from the Inference with probabilities is passed
            to the post-process function
        Returns:
            list : A list of dictionaries with predictions and explanations is returned
        """
        return data.argmax(1).tolist()
        # return data.tolist()
```

ç¬¬ 4 æ­¥ï¼šä½¿ç”¨ torch-model-archiver ç¨‹åºåˆ›å»ºä¸€ä¸ª Torch æ¨¡å‹å­˜æ¡£ä»¥å­˜æ¡£ä¸Šè¿°æ–‡ä»¶ã€‚

``` bash
$ torch-model-archiver -h
usage: torch-model-archiver [-h] --model-name MODEL_NAME  --version MODEL_VERSION_NUMBER
                      --model-file MODEL_FILE_PATH --serialized-file MODEL_SERIALIZED_PATH
                      --handler HANDLER [--runtime {python,python2,python3}]
                      [--export-path EXPORT_PATH] [-f] [--requirements-file]

Model Archiver Tool

optional arguments:
  -h, --help            show this help message and exit
  --model-name MODEL_NAME
                        Exported model name. Exported file will be named as
                        model-name.mar and saved in current working directory
                        if no --export-path is specified, else it will be
                        saved under the export path
  --serialized-file SERIALIZED_FILE
                        Path to .pt or .pth file containing state_dict in
                        case of eager mode or an executable ScriptModule
                        in case of TorchScript.
  --model-file MODEL_FILE
                        Path to python file containing model architecture.
                        This parameter is mandatory for eager mode models.
                        The model architecture file must contain only one
                        class definition extended from torch.nn.modules.
  --handler HANDLER     TorchServe's default handler name  or handler python
                        file path to handle custom TorchServe inference logic.
  --extra-files EXTRA_FILES
                        Comma separated path to extra dependency files.
  --runtime {python,python2,python3}
                        The runtime specifies which language to run your
                        inference code on. The default runtime is
                        RuntimeType.PYTHON. At the present moment we support
                        the following runtimes python, python2, python3
  --export-path EXPORT_PATH
                        Path where the exported .mar file will be saved. This
                        is an optional parameter. If --export-path is not
                        specified, the file will be saved in the current
                        working directory.
  --archive-format {tgz,default}
                        The format in which the model artifacts are archived.
                        "tgz": This creates the model-archive in <model-name>.tar.gz format.
                        If platform hosting requires model-artifacts to be in ".tar.gz"
                        use this option.
                        "no-archive": This option creates an non-archived version of model artifacts
                        at "export-path/{model-name}" location. As a result of this choice,
                        MANIFEST file will be created at "export-path/{model-name}" location
                        without archiving these model files
                        "default": This creates the model-archive in <model-name>.mar format.
                        This is the default archiving format. Models archived in this format
                        will be readily hostable on TorchServe.
  -f, --force           When the -f or --force flag is specified, an existing
                        .mar file with same name as that provided in --model-
                        name in the path specified by --export-path will
                        overwritten
  -v, --version         Model's version.
  -r, --requirements-file
                        Path to requirements.txt file containing a list of model specific python
                        packages to be installed by TorchServe for seamless model serving.
```

``` bash
torch-model-archiver --model-name mnist --version 1.0 --model-file mnist_model.py --serialized-file mnist_sd.pt --export-path ./model_store --handler mnist_handler.py -f
```

## æ¨¡å‹éƒ¨ç½²

``` bash
$ torchserve --help
usage: torchserve [-h] [-v | --version]
                          [--start]
                          [--stop]
                          [--ts-config TS_CONFIG]
                          [--model-store MODEL_STORE]
                          [--workflow-store WORKFLOW_STORE]
                          [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]
                          [--log-config LOG_CONFIG]

torchserve

mandatory arguments:
  --model-store MODEL_STORE
                        Model store location where models can be loaded

  

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         Return TorchServe Version
  --start               Start the model-server
  --stop                Stop the model-server
  --ts-config TS_CONFIG
                        Configuration file for TorchServe

  --models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]
                        Models to be loaded using [model_name=]model_location
                        format. Location can be a HTTP URL, a model archive
                        file or directory contains model archive files in
                        MODEL_STORE.
  --log-config LOG_CONFIG
                        Log4j configuration file for TorchServe
  --ncs, --no-config-snapshots         
                        Disable snapshot feature
  --workflow-store WORKFLOW_STORE
                        Workflow store location where workflow can be loaded. Defaults to model-store
```

### å¯åŠ¨torchserveæœåŠ¡

``` bash
torchserve --start --ncs --model-store model_store --models mnist.mar
```

æ¨¡å‹å¯åŠ¨æ—¥å¿—å¦‚ä¸‹æˆªå›¾ï¼š

![ts_start](Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_start.png)

### æ¨ç†å¥åº·æ£€æŸ¥API

``` bash
curl http://localhost:8080/ping
```

å¦‚æœserveræ­£å¸¸è¿è¡Œ, å“åº”ä¼šå¦‚æˆªå›¾æ‰€ç¤ºï¼š

![ts_ping](Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_ping.png)

### æ¨ç†

``` bash
curl http://127.0.0.1:8080/predictions/mnist -T ./data/test.png
```

test.pngä¸ºæ•°å­—ä¸º0çš„å›¾ç‰‡ï¼Œé€šè¿‡ä¸Šè¿°çš„è°ƒç”¨æ¨ç†ï¼Œå¯ä»¥çœ‹å‡ºç»“æœæ˜¯èƒ½æ­£å¸¸è¿”å›çš„ï¼Œæ˜¯å¯ä»¥ä½œä¸ºä¸‹æ¸¸åº”ç”¨è°ƒç”¨ã€‚

![ts_infer](Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/ts_infer.png)

### åœæ­¢torchserveæœåŠ¡

``` bash
torchserve --start
```

## æ¢ç´¢

[å®Œæ•´ä»£ç ](https://github.com/yqstar/Awesome_Pytorch_Tutorial/tree/master/Pytorch_Lesson3)å·²ä¸Šä¼ Githubï¼Œæœ‰éœ€è¦çš„å¯ä»¥è‡ªè¡Œä¸‹è½½ä»£ç ï¼Œå¦‚æœå¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·Starï¼Œå“ˆå“ˆå“ˆå“ˆï¼

åˆ°æ­¤ä¸ºæ­¢ï¼Œå·²ç»å¯ä»¥ä½¿ç”¨è‡ªå·±æ•°æ®ç©è€å„ç§Demoï¼Œå¿«ï¼ˆè‹¦ï¼‰ä¹ï¼ˆé€¼ï¼‰åœ°è¿›è¡Œç‚¼ä¸¹ä¹‹è·¯ã€‚é“è·¯é˜»ä¸”é•¿ï¼Œè¡Œåˆ™å°†è‡³ï¼Œä½†è¡Œå¥½äº‹è«é—®å‰ç¨‹ã€‚

* é™¤äº†ä½¿ç”¨TorchServeéƒ¨ç½²æ¨¡å‹ï¼Œè¿˜æœ‰å…¶ä»–çš„è§£å†³æ–¹æ¡ˆå—ï¼Ÿ
* é™¤äº†ä½¿ç”¨æä¾›è¿™ç§Web APIçš„å½¢å¼ï¼Œæ˜¯å¦å¯ä»¥æ„å»ºä¸€ä¸ªGUIçš„å½¢å¼æä¾›å‘¢ï¼Ÿä¾‹å¦‚ PYQT5 ï¼Ÿè¿™é‡Œæ”¾ä¸€å¼ PYQT5çš„å›¾ï¼Œåé¢ä¼šå¡«å‘ã€‚

![pyqt_demo](Pytorchç³»åˆ—è‡ªå­¦æ•™ç¨‹-3-æ·±åº¦å­¦ä¹ ä¹‹Mnistå›¾åƒåˆ†ç±»TorchServeéƒ¨ç½²/pyqt_demo.png)

æ­£å¦‚ï¼Œäººå¾€å¾€ä¼šå¯¹æœªçŸ¥çš„äº‹æƒ…äº§ç”Ÿææƒ§ï¼Œå› ä¸ºç»“å±€æ˜¯æœªçŸ¥çš„ã€‚æ‰€ä»¥å½“ä¸€åˆ‡ä¸å†æœªçŸ¥çš„æ—¶å€™ï¼Œé‚£ä¹ˆæ˜¯ä¸æ˜¯å°±ä¸ä¼šäº§ç”Ÿææƒ§å‘¢ï¼Ÿ

## å‚è€ƒ

More info: [Chengluï¼šå¦‚ä½•éƒ¨ç½²PyTorchæ¨¡å‹](https://zhuanlan.zhihu.com/p/344364948)
More info: [TorchServe](https://github.com/pytorch/serve)
More info: [TorchServe_Mnist Example](https://github.com/pytorch/serve/tree/master/examples/image_classifier/mnist)
More info: [éšä¾¿å†™ç‚¹ç¬”è®°](https://blog.csdn.net/weixin_41977938/article/details/122258595)
More info: [PyTorch Eager mode and Script mode](https://blog.csdn.net/Chris_zhangrx/article/details/117380516)
More info: [Self-host your ğŸ¤—HuggingFace Transformer NER model with Torchserve + Streamlit](https://cceyda.github.io/blog/huggingface/torchserve/streamlit/ner/2020/10/09/huggingface_streamlit_serve.html)
More info: [TorchServeæ­å»ºcodeBERTåˆ†ç±»æ¨¡å‹æœåŠ¡](https://ceshiren.com/t/topic/20770)
More info: [torchserveræ¨¡å‹æœ¬åœ°éƒ¨ç½²å’Œdockeréƒ¨ç½²](https://blog.csdn.net/qq_15821487/article/details/122684773)
